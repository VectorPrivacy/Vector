//! MLS (Message Layer Security) Module
//! 
//! This module provides MLS group messaging capabilities using the nostr-mls crate.
//! We use nostr-mls defaults and communicate exclusively through TRUSTED_RELAY.
//! 
//! ## Storage Schema
//!
//! This module manages the following JSON store keys:
//!
//! ### Encrypted Keys (use crypto::internal_encrypt/decrypt)
//! - "mls_groups": Array of group metadata objects
//!   ```json
//!   [{
//!     "group_id": "...",
//!     "creator_pubkey": "...",
//!     "name": "...",
//!     "avatar_ref": "...",
//!     "created_at": 1234567890,
//!     "updated_at": 1234567890
//!   }]
//!   ```
//!
//! ### Plaintext Keys
//! - "mls_keypackage_index": Array tracking device keypackages
//!   ```json
//!   [{
//!     "owner_pubkey": "...",
//!     "device_id": "...",
//!     "keypackage_ref": "...",
//!     "fetched_at": 1234567890,
//!     "expires_at": 1234567890
//!   }]
//!   ```
//!
//! - "mls_event_cursors": Object mapping group_id to sync state
//!   ```json
//!   {
//!     "group_id": {
//!       "last_seen_event_id": "...",
//!       "last_seen_at": 1234567890
//!     }
//!   }
//!   ```
//!
//! ### Messages
//! Messages are stored in the unified Chat storage (see chat.rs), not in MLS-specific storage.
//! This allows protocol-agnostic message handling across DMs and MLS groups.

use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use mdk_core::prelude::*;
use mdk_sqlite_storage::MdkSqliteStorage;
use std::sync::Arc;
use tauri::{AppHandle, Runtime, Manager, Emitter};
use tauri::path::BaseDirectory;
use crate::{TAURI_APP, NOSTR_CLIENT, TRUSTED_RELAY, STATE};
use crate::db;
use crate::crypto;
use crate::rumor::{RumorEvent, RumorContext, ConversationType, process_rumor, RumorProcessingResult};
use crate::db_migration::{save_chat, save_chat_messages};

/// MLS-specific error types following this crate's error style
#[derive(Debug)]
pub enum MlsError {
    NotInitialized,
    InvalidGroupId,
    InvalidKeyPackage,
    GroupNotFound,
    MemberNotFound,
    StorageError(String),
    NetworkError(String),
    CryptoError(String),
    NostrMlsError(String),
}

impl std::fmt::Display for MlsError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            MlsError::NotInitialized => write!(f, "MLS service not initialized"),
            MlsError::InvalidGroupId => write!(f, "Invalid group ID"),
            MlsError::InvalidKeyPackage => write!(f, "Invalid key package"),
            MlsError::GroupNotFound => write!(f, "Group not found"),
            MlsError::MemberNotFound => write!(f, "Member not found"),
            MlsError::StorageError(e) => write!(f, "Storage error: {}", e),
            MlsError::NetworkError(e) => write!(f, "Network error: {}", e),
            MlsError::CryptoError(e) => write!(f, "Crypto error: {}", e),
            MlsError::NostrMlsError(e) => write!(f, "Nostr MLS error: {}", e),
        }
    }
}

impl std::error::Error for MlsError {}

/// MLS group metadata stored encrypted in "mls_groups"
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MlsGroupMetadata {
    // Wire identifier used on the relay (wrapper 'h' tag). UI lists this value.
    pub group_id: String,
    // Engine identifier used locally by nostr-mls for group state lookups.
    // Backwards compatible with existing data via serde default.
    #[serde(default)]
    pub engine_group_id: String,
    pub creator_pubkey: String,
    pub name: String,
    pub avatar_ref: Option<String>,
    pub created_at: u64,
    pub updated_at: u64,
    // Flag indicating if we were evicted/kicked from this group
    // When true, we skip syncing this group (unless it's a new welcome/invite)
    #[serde(default)]
    pub evicted: bool,
}

/// Keypackage index entry stored in "mls_keypackage_index"
#[derive(Debug, Clone, Serialize, Deserialize)]
struct KeyPackageIndexEntry {
    owner_pubkey: String,
    device_id: String,
    keypackage_ref: String,
    fetched_at: u64,
    expires_at: u64,
}

/// Event cursor tracking for a group stored in "mls_event_cursors"
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EventCursor {
    last_seen_event_id: String,
    last_seen_at: u64,
}

/// Message record for persisting decrypted MLS messages

/// Main MLS service facade
/// 
/// Responsibilities:
/// - Initialize and manage MLS groups using nostr-mls
/// - Handle device keypackage publishing and management
/// - Process incoming MLS events from nostr relays
/// - Manage encrypted group metadata and message storage
pub struct MlsService {
    /// Persistent MLS engine when initialized (SQLite-backed via mdk-sqlite-storage)
    engine: Option<Arc<MDK<MdkSqliteStorage>>>,
    _initialized: bool,
}

impl MlsService {
    /// Create a new MLS service instance (no engine initialized)
    pub fn new() -> Self {
        Self {
            engine: None,
            _initialized: false,
        }
    }

    /// Create a new MLS service with persistent SQLite-backed storage at:
    ///   [AppData]/vector/mls/vector-mls.db
    pub fn new_persistent<R: Runtime>(handle: &AppHandle<R>) -> Result<Self, MlsError> {
        // Resolve the DB path under OS app data directory
        // Final path: [AppData]/mls/vector-mls.db
        let db_path = handle
            .path()
            .resolve("mls/vector-mls.db", BaseDirectory::AppData)
            .map_err(|e| MlsError::StorageError(format!("resolve app data dir: {}", e)))?;

        // Ensure parent directory exists before opening SQLite
        if let Some(parent) = db_path.parent() {
            if let Err(e) = std::fs::create_dir_all(parent) {
                return Err(MlsError::StorageError(format!("create mls dir: {}", e)));
            }
        }

        // Initialize persistent storage and engine
        let storage = MdkSqliteStorage::new(&db_path)
            .map_err(|e| MlsError::StorageError(format!("init sqlite storage: {}", e)))?;
        let mdk = MDK::new(storage);

        Ok(Self {
            engine: Some(Arc::new(mdk)),
            _initialized: true,
        })
    }

    /// Get a clone of the persistent MLS engine (Arc)
    pub fn engine(&self) -> Result<Arc<MDK<MdkSqliteStorage>>, MlsError> {
        self.engine.clone().ok_or(MlsError::NotInitialized)
    }

    /// Publish the device's keypackage to enable others to add this device to groups
    /// 
    /// This will:
    /// 1. Generate a new keypackage for the device if needed
    /// 2. Publish it to TRUSTED_RELAY via nostr-mls
    /// 3. Update "mls_keypackage_index" with the reference
    pub async fn publish_device_keypackage(&self, device_id: &str) -> Result<(), MlsError> {
        // TODO: Use nostr-mls to generate and publish keypackage
        // TODO: Store keypackage reference in "mls_keypackage_index"
        // TODO: Use TRUSTED_RELAY for publishing
        
        // Stub implementation
        let _ = device_id;
        Ok(())
    }

    /// Create a new MLS group
    /// 
    /// This will:
    /// 1. Create the group using nostr-mls
    /// 2. Add initial member devices
    /// 3. Store encrypted metadata in "mls_groups" using crypto::internal_encrypt
    /// 4. Initialize per-group message storage keys
    /*
    Flow and error surfaces for persistent group creation (UI-visible behavior)
    - Inputs:
      • name: UI-supplied group name (validated in [rust.create_group_chat()](src-tauri/src/lib.rs:3108))
      • avatar_ref: not used for this subtask (None)
      • initial_member_devices: Vec of (member_npub, device_id) pairs chosen by the caller

    - Steps:
      1) Resolve creator pubkey and build NostrGroupConfigData scoped to TRUSTED_RELAY.
      2) Resolve each member device to its KeyPackage Event before touching the MLS engine:
         • Prefer local plaintext index "mls_keypackage_index" to get keypackage_ref by member npub + device_id.
         • If ref exists: fetch exact event by id; else: fetch latest Kind::MlsKeyPackage by author.
         • Any member device with no resolvable KeyPackage is skipped here (this is a safe-guard; the UI path pre-validates via [rust.create_group_chat()](src-tauri/src/lib.rs:3108) and should not reach here with missing devices).
      3) Create the group with the persistent sqlite-backed engine (no await while engine is in scope):
         • engine.create_group(my_pubkey, member_kp_events, admins=[my_pubkey], group_config)
         • Capture:
           - engine_group_id (internal engine id, hex) for local operations and send path.
           - wire group id used on relays (h tag). We derive a canonical 64-hex when possible; fallback to engine id.
      4) Publish welcome(s) to invited recipients 1:1 via gift_wrap_to on TRUSTED_RELAY.
      5) Persist encrypted UI metadata ("mls_groups") with:
         • group_id = wire id (relay filtering id, shown in UI)
         • engine_group_id = engine id (used by [rust.send_mls_group_message()](src-tauri/src/lib.rs:3144))
      6) Emit "mls_group_initial_sync" immediately so the frontend can refresh chat list without restart.

    - Error mapping (propagated as strings to the UI):
      • MlsError::NotInitialized: Nostr client/app handle not ready.
      • MlsError::NetworkError: signer resolution, relay parsing, or network fetch/publish failures.
      • MlsError::NostrMlsError: engine create_group/create_message failures (e.g., storage/codec issues).
      • MlsError::StorageError: reading/writing JSON store or sqlite engine initialization paths.
      • MlsError::CryptoError: bech32 conversions or encrypted store (de)serialization.
      These are returned as Err(String) up to [rust.create_group_chat()](src-tauri/src/lib.rs:3108) and surfaced verbatim by the UI.

    - Persistence & discoverability:
      • The group metadata is written to "mls_groups" (encrypted) so it appears in list_mls_groups().
      • The frontend awaits loadMLSGroups() and opens the chat (openChat(group_id)) immediately.
      • Event "mls_group_initial_sync" is emitted here for zero-latency list refresh.

    - Partial membership:
      • If some members had no resolvable KeyPackage at engine time, they are skipped here; however, the preflight in [rust.create_group_chat()](src-tauri/src/lib.rs:3108) aborts early on any missing device, ensuring atomic creation semantics for the UI flow.
    */
    pub async fn create_group(
        &self,
        name: &str,
        avatar_ref: Option<&str>,
        initial_member_devices: &[(String, String)], // (member_pubkey, device_id) pairs
    ) -> Result<String, MlsError> {
        // Persistent group creation using sqlite-backed engine.
        // - Resolve signer and relay config
        // - Use engine.create_group() inside a no-await scope (avoid holding !Send across await)
        // - Publish welcome (if any) to TRUSTED_RELAY
        // - Store encrypted UI metadata to "mls_groups"
        //
        // TODO: Resolve `initial_member_devices` into Vec<Event> KeyPackages (from index or network).

        // Resolve client and my pubkey
        let client = NOSTR_CLIENT.get().ok_or(MlsError::NotInitialized)?;
        let signer = client
            .signer()
            .await
            .map_err(|e| MlsError::NetworkError(e.to_string()))?;
        let my_pubkey = signer
            .get_public_key()
            .await
            .map_err(|e| MlsError::NetworkError(e.to_string()))?;
        let creator_pubkey_b32 = my_pubkey
            .to_bech32()
            .map_err(|e| MlsError::CryptoError(e.to_string()))?;

        // Build group config (relay-scoped)
        let relay_url = RelayUrl::parse(TRUSTED_RELAY)
            .map_err(|e| MlsError::NetworkError(format!("RelayUrl::parse: {}", e)))?;
        let description = format!("Vector group: {}", name);
        let group_config = NostrGroupConfigData::new(
            name.to_string(),
            description,
            None, // image_hash
            None, // image_key
            None, // image_nonce
            vec![relay_url],
            vec![my_pubkey], // admins - moved from create_group call
        );

        // Resolve member KeyPackage events before engine usage (awaits allowed here)
        use nostr_sdk::prelude::*;
        let mut member_kp_events: Vec<Event> = Vec::new();
        let mut invited_recipients: Vec<PublicKey> = Vec::new();

        // Load plaintext index
        let index = self.read_keypackage_index().await.unwrap_or_default();

        for (member_npub, device_id) in initial_member_devices.iter() {
            // Parse target public key (used for later gift-wrapping)
            let member_pk = match PublicKey::from_bech32(member_npub) {
                Ok(pk) => pk,
                Err(_) => {
                    eprintln!("[MLS] Invalid member npub: {}", member_npub);
                    continue;
                }
            };

            // Try index first
            let mut ref_event_id_hex: Option<String> = None;
            for entry in &index {
                if entry.owner_pubkey == *member_npub && entry.device_id == *device_id {
                    ref_event_id_hex = Some(entry.keypackage_ref.clone());
                    break;
                }
            }

            let kp_event: Option<Event> = if let Some(id_hex) = ref_event_id_hex {
                // Fetch exact event by id from TRUSTED_RELAY
                let id = match EventId::from_hex(&id_hex) {
                    Ok(v) => v,
                    Err(_) => {
                        println!("[MLS] Invalid keypackage_ref in index for {}:{}", member_npub, device_id);
                        continue;
                    }
                };
                let filter = Filter::new().id(id).limit(1);
                match NOSTR_CLIENT
                    .get()
                    .unwrap()
                    .fetch_events_from(vec![TRUSTED_RELAY], filter, std::time::Duration::from_secs(10))
                    .await
                {
                    Ok(events) => events.into_iter().next(),
                    Err(e) => {
                        eprintln!("[MLS] Fetch KeyPackage by id failed ({}:{}): {}", member_npub, device_id, e);
                        None
                    }
                }
            } else {
                // Fallback: fetch latest KeyPackage by author from TRUSTED_RELAY
                let filter = Filter::new()
                    .author(member_pk)
                    .kind(Kind::MlsKeyPackage)
                    .limit(50);
                match NOSTR_CLIENT
                    .get()
                    .unwrap()
                    .fetch_events_from(vec![TRUSTED_RELAY], filter, std::time::Duration::from_secs(10))
                    .await
                {
                    Ok(events) => {
                        // Heuristic: pick newest by created_at
                        let selected = events.into_iter().max_by_key(|e| e.created_at.as_u64());
                        if selected.is_none() {
                            eprintln!("[MLS] No KeyPackage events found for {}", member_npub);
                        }
                        selected
                    }
                    Err(e) => {
                        eprintln!("[MLS] Fetch KeyPackages for {} failed: {}", member_npub, e);
                        None
                    }
                }
            };

            if let Some(ev) = kp_event {
                member_kp_events.push(ev);
                invited_recipients.push(member_pk);
            } else {
                // Continue without this member device (will create group without them)
                eprintln!("[MLS] Skipping member device {}:{} (no KeyPackage event)", member_npub, device_id);
            }
        }

        let invited_count = member_kp_events.len();

        // Perform engine operations without awaits in scope
        let (group_id_hex, engine_gid_hex, welcome_rumors) = {
            let engine = self.engine()?; // Arc to sqlite engine (may be !Send internally)
            let create_out = engine
                .create_group(
                    &my_pubkey,
                    member_kp_events,              // invited devices' keypackage events
                    group_config,                  // admins now in config
                )
                .map_err(|e| MlsError::NostrMlsError(format!("create_group: {}", e)))?;

            // GroupId is already a GroupId type in MDK (no conversion needed)
            let gid_bytes = create_out.group.mls_group_id.as_slice();
            let engine_gid_hex = hex::encode(gid_bytes);

            // Attempt to derive wire id (wrapper 'h' tag, 64-hex) using a non-published dummy wrapper.
            // If unavailable, fall back to engine id.
            let wire_gid_hex = {
                use nostr_sdk::prelude::*;
                let dummy_rumor = EventBuilder::new(Kind::Custom(9), "vector-mls-bootstrap")
                    .tag(Tag::custom(
                        TagKind::Custom(std::borrow::Cow::Borrowed("vector-mls-bootstrap")),
                        vec!["true"],
                    ))
                    .build(*&my_pubkey);
                if let Ok(wrapper) = engine.create_message(&create_out.group.mls_group_id, dummy_rumor) {
                    if let Some(h_tag) = wrapper
                        .tags
                        .find(TagKind::SingleLetter(SingleLetterTag::lowercase(Alphabet::H)))
                    {
                        if let Some(canon) = h_tag.content() {
                            if canon.len() == 64 {
                                canon.to_string()
                            } else {
                                // Fallback to engine id if 'h' content is unexpected
                                engine_gid_hex.clone()
                            }
                        } else {
                            engine_gid_hex.clone()
                        }
                    } else {
                        engine_gid_hex.clone()
                    }
                } else {
                    engine_gid_hex.clone()
                }
            };

            // Use wire id for UI/store group_id (relay filtering), engine id for local engine ops.
            let gid_hex = wire_gid_hex;

            (gid_hex, engine_gid_hex, create_out.welcome_rumors)
        }; // engine dropped here before any await

        // Accept 32-hex or 64-hex group ids (engine/codec variability)
        if group_id_hex.len() != 32 && group_id_hex.len() != 64 {
            eprintln!(
                "[MLS] create_group: unexpected group_id length={}, proceeding may affect relay filtering",
                group_id_hex.len()
            );
        }

        // Publish welcomes (gift-wrapped) 1:1 with invited recipients where possible
        if !welcome_rumors.is_empty() {
            if welcome_rumors.len() != invited_count {
                eprintln!(
                    "[MLS] welcome/member count mismatch: welcomes={}, invited={}",
                    welcome_rumors.len(),
                    invited_count
                );
            }
            let min_len = std::cmp::min(welcome_rumors.len(), invited_recipients.len());
            for i in 0..min_len {
                let welcome = welcome_rumors[i].clone(); // UnsignedEvent
                let target = invited_recipients[i];
                match NOSTR_CLIENT
                    .get()
                    .unwrap()
                    .gift_wrap_to([TRUSTED_RELAY], &target, welcome, [])
                    .await
                {
                    Ok(wrapper_id) => {
                        let recipient = target.to_bech32().unwrap_or_default();
                        println!(
                            "[MLS][welcome][published] wrapper_id={}, recipient={}, relay={}",
                            wrapper_id.to_hex(),
                            recipient,
                            TRUSTED_RELAY
                        );
                    }
                    Err(e) => {
                        let recipient = target.to_bech32().unwrap_or_default();
                        eprintln!(
                            "[MLS][welcome][publish_error] recipient={}, relay={}, err={}",
                            recipient,
                            TRUSTED_RELAY,
                            e
                        );
                    }
                }
            }
        } else {
            println!(
                "[MLS] No welcome rumors (invited={}, self-only path likely)",
                invited_count
            );
        }

        // Persist encrypted "mls_groups"
        let now_secs = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map_err(|e| MlsError::StorageError(format!("system time error: {}", e)))?
            .as_secs();

        let meta = MlsGroupMetadata {
            group_id: group_id_hex.clone(),        // wire id for UI/filtering
            engine_group_id: engine_gid_hex,       // engine id for local operations
            creator_pubkey: creator_pubkey_b32,
            name: name.to_string(),
            avatar_ref: avatar_ref.map(|s| s.to_string()),
            created_at: now_secs,
            updated_at: now_secs,
            evicted: false,                        // New groups are not evicted
        };

        let mut groups = self.read_groups().await?;
        groups.push(meta.clone());
        self.write_groups(&groups).await?;
 
        // Create the Chat in STATE with metadata and save to disk
        {
            let mut state = STATE.lock().await;
            let chat_id = state.create_or_get_mls_group_chat(&group_id_hex, vec![]);
            
            // Set metadata from MlsGroupMetadata
            if let Some(chat) = state.get_chat_mut(&chat_id) {
                chat.metadata.set_name(meta.name.clone());
                chat.metadata.set_member_count(invited_count + 1); // +1 for creator
            }
            
            // Save chat to disk
            if let Some(handle) = TAURI_APP.get() {
                let handle_clone = handle.clone();
                if let Some(chat) = state.get_chat(&chat_id) {
                    if let Err(e) = save_chat(handle_clone, chat).await {
                        eprintln!("[MLS] Failed to save chat after group creation: {}", e);
                    }
                }
            }
        }
 
        // Notify UI: reuse the same event used after welcome-accept so creator also sees the group immediately.
        if let Some(handle) = TAURI_APP.get() {
            handle.emit("mls_group_initial_sync", serde_json::json!({
                "group_id": group_id_hex,
                "processed": 0u32,
                "new": 0u32
            })).unwrap_or_else(|e| {
                eprintln!("[MLS] Failed to emit mls_group_initial_sync after create: {}", e);
            });
        }
 
        println!(
            "[MLS] Created group (persistent) id={}, name=\"{}\", invited_devices_hint={}",
            group_id_hex,
            name,
            initial_member_devices.len()
        );
        Ok(group_id_hex)
    }

    /// Add a member device to an existing group
    ///
    /// This will:
    /// 1. Fetch the device's keypackage from the network
    /// 2. Add the device to the group via nostr-mls
    /// 3. Send the welcome message
    /// 4. Update group metadata
    pub async fn add_member_device(
        &self,
        group_id: &str,
        member_pubkey: &str,
        device_id: &str,
    ) -> Result<(), MlsError> {
        use nostr_sdk::prelude::*;

        // Resolve client
        let client = NOSTR_CLIENT.get().ok_or(MlsError::NotInitialized)?;

        // Parse member public key
        let member_pk = PublicKey::from_bech32(member_pubkey)
            .map_err(|e| MlsError::CryptoError(format!("Invalid member npub: {}", e)))?;

        // Fetch member's keypackage from index or network
        let index = self.read_keypackage_index().await.unwrap_or_default();
        let mut kp_event: Option<Event> = None;

        // Try index first
        for entry in &index {
            if entry.owner_pubkey == member_pubkey && entry.device_id == device_id {
                let id = EventId::from_hex(&entry.keypackage_ref)
                    .map_err(|e| MlsError::CryptoError(format!("Invalid keypackage ref: {}", e)))?;
                let filter = Filter::new().id(id).limit(1);
                if let Ok(events) = client
                    .fetch_events_from(vec![TRUSTED_RELAY], filter, std::time::Duration::from_secs(10))
                    .await
                {
                    kp_event = events.into_iter().next();
                }
                break;
            }
        }

        // Fallback: fetch latest from network
        if kp_event.is_none() {
            let filter = Filter::new()
                .author(member_pk)
                .kind(Kind::MlsKeyPackage)
                .limit(50);
            if let Ok(events) = client
                .fetch_events_from(vec![TRUSTED_RELAY], filter, std::time::Duration::from_secs(10))
                .await
            {
                kp_event = events.into_iter().max_by_key(|e| e.created_at.as_u64());
            }
        }

        let kp_event = kp_event.ok_or_else(|| {
            MlsError::NetworkError(format!("No keypackage found for {}:{}", member_pubkey, device_id))
        })?;

        // Find the group's MLS group ID
        let groups = self.read_groups().await?;
        let group_meta = groups.iter()
            .find(|g| g.group_id == group_id || g.engine_group_id == group_id)
            .ok_or(MlsError::GroupNotFound)?;
        
        // Convert engine_group_id hex to GroupId
        let mls_group_id = GroupId::from_slice(
            &hex::decode(&group_meta.engine_group_id)
                .map_err(|e| MlsError::CryptoError(format!("Invalid group ID hex: {}", e)))?
        );

        // Perform engine operations (add member but DON'T merge yet)
        let (evolution_event, welcome_rumors) = {
            let engine = self.engine()?;
            
            // Add member to group - returns AddMembersResult with evolution_event and welcome_rumors
            let add_result = engine
                .add_members(&mls_group_id, std::slice::from_ref(&kp_event))
                .map_err(|e| {
                    eprintln!("[MLS] Failed to add member: {}", e);
                    MlsError::NostrMlsError(format!("Failed to add member: {}", e))
                })?;

            (add_result.evolution_event, add_result.welcome_rumors)
        };

        // Send welcome before merging commit (welcome is created for current epoch)
        if let Some(welcome_rumors) = welcome_rumors {
            for welcome in welcome_rumors {
                if let Err(e) = client.gift_wrap_to([TRUSTED_RELAY], &member_pk, welcome, []).await {
                    eprintln!("[MLS] Failed to send welcome: {}", e);
                }
            }
        }

        // Publish evolution event (commit) to the relay
        if let Err(e) = client.send_event(&evolution_event).await {
            eprintln!("[MLS] Failed to publish commit: {}", e);
        }

        // NOW merge the pending commit after welcome and evolution event are sent
        {
            let engine = self.engine()?;
            engine
                .merge_pending_commit(&mls_group_id)
                .map_err(|e| {
                    eprintln!("[MLS] Failed to merge commit: {}", e);
                    MlsError::NostrMlsError(format!("Failed to merge commit: {}", e))
                })?;
        }

        // Update group metadata timestamp
        let mut groups = self.read_groups().await?;
        if let Some(group) = groups.iter_mut().find(|g| g.group_id == group_id) {
            group.updated_at = std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs();
            self.write_groups(&groups).await?;
        }

        // Emit event to refresh UI
        if let Some(handle) = TAURI_APP.get() {
            handle.emit("mls_group_updated", serde_json::json!({
                "group_id": group_id
            })).ok();
        }

        Ok(())
    }


    /// Leave a group voluntarily
    ///
    /// This will:
    /// 1. Create a leave proposal using MDK's leave_group()
    /// 2. Publish the evolution event to the relay
    /// 3. Remove the group from local metadata
    ///
    /// Note: The leave creates a proposal that needs to be committed by an admin
    pub async fn leave_group(&self, group_id: &str) -> Result<(), MlsError> {
        use nostr_sdk::prelude::*;

        // Resolve client
        let client = NOSTR_CLIENT.get().ok_or(MlsError::NotInitialized)?;

        // Find the group's MLS group ID
        let groups = self.read_groups().await?;
        let group_meta = groups.iter()
            .find(|g| g.group_id == group_id || g.engine_group_id == group_id)
            .ok_or(MlsError::GroupNotFound)?;
        
        // Convert engine_group_id hex to GroupId
        let mls_group_id = GroupId::from_slice(
            &hex::decode(&group_meta.engine_group_id)
                .map_err(|e| MlsError::CryptoError(format!("Invalid group ID hex: {}", e)))?
        );

        // Perform engine operation (leave group)
        let evolution_event = {
            let engine = self.engine()?;
            
            // Leave the group - returns LeaveGroupResult with evolution_event
            let leave_result = engine
                .leave_group(&mls_group_id)
                .map_err(|e| {
                    eprintln!("[MLS] Failed to leave group: {}", e);
                    MlsError::NostrMlsError(format!("Failed to leave group: {}", e))
                })?;

            leave_result.evolution_event
        };

        // Publish the evolution event (leave proposal) to the relay
        if let Err(e) = client.send_event(&evolution_event).await {
            eprintln!("[MLS] Failed to publish leave proposal: {}", e);
        }

        // Remove the group from local metadata
        let mut groups = self.read_groups().await?;
        groups.retain(|g| g.group_id != group_id && g.engine_group_id != group_meta.engine_group_id);
        self.write_groups(&groups).await?;

        // Emit event to refresh UI
        if let Some(handle) = TAURI_APP.get() {
            handle.emit("mls_group_left", serde_json::json!({
                "group_id": group_id
            })).ok();
        }

        Ok(())
    }

    /// Remove a member device from a group (admin only)
    ///
    /// This will:
    /// 1. Remove the member using MDK's remove_members()
    /// 2. Publish the commit message to remaining group members
    /// 3. Merge the pending commit locally
    /// 4. Emit UI update event
    pub async fn remove_member_device(
        &self,
        group_id: &str,
        member_pubkey: &str,
        _device_id: &str,
    ) -> Result<(), MlsError> {
        use nostr_sdk::prelude::*;

        // Resolve client
        let client = NOSTR_CLIENT.get().ok_or(MlsError::NotInitialized)?;

        // Parse member pubkey
        let member_pk = PublicKey::from_bech32(member_pubkey)
            .map_err(|e| MlsError::CryptoError(format!("Invalid member pubkey: {}", e)))?;

        // Find the group's MLS group ID
        let groups = self.read_groups().await?;
        let group_meta = groups.iter()
            .find(|g| g.group_id == group_id || g.engine_group_id == group_id)
            .ok_or(MlsError::GroupNotFound)?;
        
        // Convert engine_group_id hex to GroupId
        let mls_group_id = GroupId::from_slice(
            &hex::decode(&group_meta.engine_group_id)
                .map_err(|e| MlsError::CryptoError(format!("Invalid group ID hex: {}", e)))?
        );

        // Sync the group first to ensure we have the latest state
        if let Err(e) = self.sync_group_since_cursor(group_id).await {
            eprintln!("[MLS] Failed to sync group before removal: {}", e);
        }
        
        // Perform engine operation (remove member but DON'T merge yet)
        let evolution_event = {
            let engine = self.engine()?;
            
            // Verify the member exists in the group
            let current_members = engine.get_members(&mls_group_id)
                .map_err(|e| {
                    eprintln!("[MLS] Failed to get current members: {}", e);
                    MlsError::NostrMlsError(format!("Failed to get group members: {}", e))
                })?;
            
            if !current_members.contains(&member_pk) {
                eprintln!("[MLS] Member {} not found in group", member_pubkey);
                return Err(MlsError::NostrMlsError(
                    "Member not found in group. The group state may be out of sync.".to_string()
                ));
            }
            
            // Remove member from group - returns RemoveMembersResult with evolution_event
            let remove_result = engine
                .remove_members(&mls_group_id, &[member_pk])
                .map_err(|e| {
                    eprintln!("[MLS] Failed to remove member: {}", e);
                    MlsError::NostrMlsError(format!("Failed to remove member: {}", e))
                })?;

            remove_result.evolution_event
        };

        // Publish evolution event (commit) to the relay
        match client.send_event(&evolution_event).await {
            Ok(_) => {}
            Err(e) => {
                eprintln!("[MLS] Failed to publish commit: {}", e);
                return Err(MlsError::NetworkError(format!("Failed to publish commit: {}", e)));
            }
        }

        // NOW merge the pending commit after evolution event is sent
        {
            let engine = self.engine()?;
            engine
                .merge_pending_commit(&mls_group_id)
                .map_err(|e| {
                    eprintln!("[MLS] Failed to merge commit: {}", e);
                    MlsError::NostrMlsError(format!("Failed to merge commit: {}", e))
                })?;
        }

        // Emit event to refresh UI member list
        if let Some(handle) = TAURI_APP.get() {
            handle.emit("mls_group_updated", serde_json::json!({
                "group_id": group_id
            })).ok();
        }
        Ok(())
    }

    /// Send a message to an MLS group
    ///
    /// This will:
    /// 1. Encrypt the message for all group members via nostr-mls
    /// 2. Publish to TRUSTED_RELAY
    /// 3. Optionally store in "mls_messages_{group_id}" for optimistic UI
    /// 4. Update "mls_timeline_{group_id}"
    pub async fn send_group_message(
        &self,
        group_id: &str,
        text: &str,
        replied_to: Option<String>,
    ) -> Result<String, MlsError> {
        use nostr_sdk::prelude::*;

        // Create a pending message immediately for optimistic UI
        let current_time = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap();
        let pending_id = format!("pending-{}", current_time.as_nanos());
        
        let pending_msg = crate::Message {
            id: pending_id.clone(),
            content: text.to_string(),
            replied_to: replied_to.clone().unwrap_or_default(),
            preview_metadata: None,
            at: current_time.as_millis() as u64,
            attachments: Vec::new(),
            reactions: Vec::new(),
            pending: true,
            failed: false,
            mine: true,
            npub: None, // Pending messages don't need npub (they're always mine)
        };
        
        // Add pending message to state and emit to UI
        {
            let mut state = crate::STATE.lock().await;
            state.add_message_to_chat(group_id, pending_msg.clone());
        }
        
        if let Some(handle) = TAURI_APP.get() {
            handle.emit("mls_message_new", serde_json::json!({
                "group_id": group_id,
                "message": pending_msg
            })).unwrap_or_else(|e| {
                eprintln!("[MLS] Failed to emit pending message: {}", e);
            });
        }

        // Resolve target metadata
        let groups = self.read_groups().await?;
        let meta = match groups.iter().find(|g| g.group_id == group_id || (!g.engine_group_id.is_empty() && g.engine_group_id == group_id)) {
            Some(m) => m.clone(),
            None => {
                eprintln!("[MLS] Group not found: {}", group_id);
                
                // Mark pending message as failed and emit full message
                {
                    let mut state = crate::STATE.lock().await;
                    if let Some(chat) = state.chats.iter_mut().find(|c| c.id == group_id) {
                        if let Some(msg) = chat.messages.iter_mut().find(|m| m.id == pending_id) {
                            msg.failed = true;
                            msg.pending = false;
                            
                            // Emit update with full message object
                            if let Some(handle) = TAURI_APP.get() {
                                handle.emit("message_update", serde_json::json!({
                                    "old_id": &pending_id,
                                    "message": msg,
                                    "chat_id": group_id
                                })).ok();
                            }
                        }
                    }
                }
                
                return Err(MlsError::GroupNotFound);
            }
        };

        // Resolve client and my pubkey (awaits before any engine usage)
        let client = NOSTR_CLIENT.get().ok_or(MlsError::NotInitialized)?;
        let signer = client
            .signer()
            .await
            .map_err(|e| MlsError::NetworkError(e.to_string()))?;
        let my_pubkey = signer
            .get_public_key()
            .await
            .map_err(|e| MlsError::NetworkError(e.to_string()))?;

        // Build a minimal inner rumor carrying the plaintext payload.
        let mut rumor_builder = EventBuilder::new(Kind::PrivateDirectMessage, text)
            .tag(Tag::custom(
                TagKind::Custom(std::borrow::Cow::Borrowed("vector-mls-msg")),
                // Attach the wire id (UI/relay id) for easier diagnostics
                vec![&meta.group_id],
            ));
        
        // Add reply tag if replying to a message
        if let Some(ref reply_id) = replied_to {
            if !reply_id.is_empty() {
                rumor_builder = rumor_builder.tag(Tag::custom(
                    TagKind::e(),
                    vec![reply_id, "", "reply"],
                ));
            }
        }
        
        let rumor = rumor_builder.build(my_pubkey);
        
        // Store the inner event id for optimistic persistence
        let inner_event_id = rumor.id.unwrap().to_hex();
        
        // Clone rumor for later processing (before it's moved into create_message)
        let rumor_for_processing = rumor.clone();

        // Create the MLS wrapper with the non-Send engine in a no-await scope
        let wrapper_event = {
            // Decode GroupId from engine id (preferred), fall back to wire id if missing in legacy data
            let engine_hex = if !meta.engine_group_id.is_empty() { &meta.engine_group_id } else { &meta.group_id };
            let gid_bytes = hex::decode(engine_hex)
                .map_err(|e| {
                    eprintln!("[MLS] Failed to decode engine_group_id hex '{}': {}", engine_hex, e);
                    MlsError::InvalidGroupId
                })?;
            let gid = GroupId::from_slice(&gid_bytes);

            let engine = self.engine()?; // Arc<NostrMls<NostrMlsSqliteStorage>>
            
            // TODO: Handle potential rekey/commit flow if required by nostr-mls
            engine
                .create_message(&gid, rumor)
                .map_err(|e| {
                    let error_msg = e.to_string();
                    eprintln!("[MLS] create_message failed for engine_group_id {}: {}", engine_hex, e);
                    eprintln!("[MLS] Error details: {:?}", e);
                    
                    // Check if this is a pending proposal error
                    if error_msg.contains("pending proposal") || error_msg.contains("PendingProposal") {
                        eprintln!("[MLS] ⚠️  PENDING PROPOSAL ERROR - Auto-cleaning broken group state");
                        eprintln!("[MLS] This group has an uncommitted proposal from a previous session");
                        eprintln!("[MLS] Removing group from local state - user will need to accept a fresh invite");
                        
                        // Note: We can't do async cleanup here in the engine scope
                        // The group will be marked as broken and user will need to accept fresh invite
                        // The error message will guide them
                        
                        return MlsError::NostrMlsError("Group has pending proposal. Please ask an admin to re-invite you.".to_string());
                    }
                    
                    eprintln!("[MLS] Group not found in engine state");
                    MlsError::NostrMlsError(format!("create_message: group not found - {}", e))
                })?
        }; // engine dropped here

        // Process the rumor through unified storage to replace pending message
        // This ensures replies and other metadata are properly extracted
        {
            use crate::rumor::{process_rumor, RumorProcessingResult, RumorContext, ConversationType};
            
            let rumor_context = RumorContext {
                sender: my_pubkey,
                is_mine: true,
                conversation_id: meta.group_id.clone(),
                conversation_type: ConversationType::MlsGroup,
            };
            
            // Convert UnsignedEvent to RumorEvent
            let rumor_event = crate::rumor::RumorEvent {
                id: rumor_for_processing.id.unwrap(),
                kind: rumor_for_processing.kind,
                content: rumor_for_processing.content.clone(),
                tags: rumor_for_processing.tags.clone(),
                created_at: rumor_for_processing.created_at,
                pubkey: rumor_for_processing.pubkey,
            };
            
            // Process the rumor to extract reply references and create proper Message
            match process_rumor(rumor_event, rumor_context).await {
                Ok(result) => {
                    match result {
                        RumorProcessingResult::TextMessage(mut msg) => {
                            // Keep the message as pending until network publish succeeds
                            msg.pending = true;
                            
                            // Remove the pending message and add the real one (still pending)
                            {
                                let mut state = crate::STATE.lock().await;
                                if let Some(chat) = state.chats.iter_mut().find(|c| c.id == meta.group_id) {
                                    // Remove pending message
                                    chat.messages.retain(|m| m.id != pending_id);
                                    // Add real message (still pending network confirmation)
                                    chat.internal_add_message(msg.clone());
                                }
                            }
                            
                            // Emit message_update to replace pending with real message (still shows as pending)
                            if let Some(handle) = TAURI_APP.get() {
                                handle.emit("message_update", serde_json::json!({
                                    "old_id": &pending_id,
                                    "message": &msg,
                                    "chat_id": &meta.group_id
                                })).unwrap_or_else(|e| {
                                    eprintln!("[MLS] Failed to emit message_update: {}", e);
                                });
                            }
                        }
                        _ => {
                            eprintln!("[MLS] Unexpected rumor processing result for text message");
                        }
                    }
                }
                Err(e) => {
                    eprintln!("[MLS] Failed to process sent rumor: {}", e);
                    
                    // Mark pending message as failed and emit full message
                    {
                        let mut state = crate::STATE.lock().await;
                        if let Some(chat) = state.chats.iter_mut().find(|c| c.id == meta.group_id) {
                            if let Some(msg) = chat.messages.iter_mut().find(|m| m.id == pending_id) {
                                msg.failed = true;
                                msg.pending = false;
                                
                                // Emit update with full message object
                                if let Some(handle) = TAURI_APP.get() {
                                    handle.emit("message_update", serde_json::json!({
                                        "old_id": &pending_id,
                                        "message": msg,
                                        "chat_id": &meta.group_id
                                    })).ok();
                                }
                            }
                        }
                    }
                }
            }
            
        }

        // Publish wrapper to TRUSTED_RELAY with retry logic (network await after engine is dropped)
        let mut send_attempts = 0;
        const MAX_ATTEMPTS: u32 = 12;
        const RETRY_DELAY: u64 = 5; // 5 seconds
        
        let mut send_success = false;
        
        while send_attempts < MAX_ATTEMPTS {
            send_attempts += 1;
            
            match client
                .send_event_to([TRUSTED_RELAY], &wrapper_event)
                .await
            {
                Ok(output) => {
                    // Check if at least one relay acknowledged the message
                    if !output.success.is_empty() {
                        send_success = true;
                        break;
                    } else if output.failed.is_empty() {
                        // No success but also no failures - temporary network issue, retry
                    } else {
                        // We have failures but no successes
                        if send_attempts == MAX_ATTEMPTS {
                            break; // Exit loop, will be handled as failure below
                        }
                    }
                    
                    // Wait before retrying if we haven't reached max attempts
                    if send_attempts < MAX_ATTEMPTS {
                        tokio::time::sleep(tokio::time::Duration::from_secs(RETRY_DELAY)).await;
                    }
                }
                Err(e) => {
                    eprintln!("[MLS] Failed to send wrapper (attempt {}/{}): {}", send_attempts, MAX_ATTEMPTS, e);
                    
                    if send_attempts == MAX_ATTEMPTS {
                        break; // Exit loop, will be handled as failure below
                    }
                    
                    // Wait before retrying
                    if send_attempts < MAX_ATTEMPTS {
                        tokio::time::sleep(tokio::time::Duration::from_secs(RETRY_DELAY)).await;
                    }
                }
            }
        }
        
        if send_success {
            // Mark message as successfully sent (no longer pending) and emit full message
            {
                let mut state = crate::STATE.lock().await;
                if let Some(chat) = state.chats.iter_mut().find(|c| c.id == meta.group_id) {
                    if let Some(msg) = chat.messages.iter_mut().find(|m| m.id == inner_event_id) {
                        msg.pending = false;
                        
                        // Emit update with full message object
                        if let Some(handle) = TAURI_APP.get() {
                            handle.emit("message_update", serde_json::json!({
                                "old_id": &inner_event_id,
                                "message": msg,
                                "chat_id": &meta.group_id
                            })).ok();
                        }
                    }
                }
            }
            
            // Return the network event id as the message identifier
            Ok(wrapper_event.id.to_hex())
        } else {
            // Failed to send after all retries
            eprintln!("[MLS] Failed to publish wrapper after {} attempts", MAX_ATTEMPTS);
            
            // Mark the message as failed and emit full message
            {
                let mut state = crate::STATE.lock().await;
                if let Some(chat) = state.chats.iter_mut().find(|c| c.id == meta.group_id) {
                    // Find the real message and mark it as failed
                    if let Some(msg) = chat.messages.iter_mut().find(|m| m.id == inner_event_id) {
                        msg.failed = true;
                        msg.pending = false;
                        
                        // Emit update with full message object
                        if let Some(handle) = TAURI_APP.get() {
                            handle.emit("message_update", serde_json::json!({
                                "old_id": &inner_event_id,
                                "message": msg,
                                "chat_id": &meta.group_id
                            })).ok();
                        }
                    }
                }
            }
            
            Err(MlsError::NetworkError("Failed to publish MLS wrapper after retries".to_string()))
        }
    }

    /// Process an incoming MLS event from the nostr network
    /// 
    /// This will:
    /// 1. Parse the nostr event containing MLS data
    /// 2. Decrypt and process via nostr-mls
    /// 3. Update relevant storage (messages, group state, etc.)
    /// 4. Update "mls_event_cursors" for the group
    /// 
    /// Returns true if the event was successfully processed
    pub async fn process_incoming_event(&self, event_json: &str) -> Result<bool, MlsError> {
        // TODO: Parse nostr event JSON
        // TODO: Extract MLS ciphertext from event
        // TODO: Process through nostr-mls (handles welcome, commit, application messages)
        // TODO: Store any resulting messages in "mls_messages_{group_id}"
        // TODO: Update "mls_event_cursors" with event ID and timestamp
        
        // Stub implementation
        let _ = event_json;
        Ok(false)
    }

    /// Sync group messages since last cursor position
    /// 
    /// This will:
    /// 1. Read cursor from "mls_event_cursors" for the group
    /// 2. Query TRUSTED_RELAY for events since cursor
    /// 3. Process each event via process_incoming_event
    /// 4. Update cursor position
    /// 
    /// Returns (processed_events_count, new_messages_count)
    pub async fn sync_group_since_cursor(&self, group_id: &str) -> Result<(u32, u32), MlsError> {
        use nostr_sdk::prelude::*;

        if group_id.is_empty() {
            return Err(MlsError::InvalidGroupId);
        }

        // 1) Check if this group is marked as evicted
        let groups = self.read_groups().await.ok();
        let group_metadata = groups.as_ref().and_then(|gs| {
            gs.iter().find(|g| g.group_id == group_id || (!g.engine_group_id.is_empty() && g.engine_group_id == group_id))
        });
        
        if let Some(meta) = group_metadata {
            if meta.evicted {
                return Ok((0, 0)); // Skip sync for evicted group
            }
        }

        // 2) Load last cursor and compute since/until window
        let mut cursors = self.read_event_cursors().await.unwrap_or_default();

        let now = Timestamp::now();
        
        let since = if let Some(cur) = cursors.get(group_id) {
            Timestamp::from_secs(cur.last_seen_at)
        } else {
            // No cursor: default to last 48h for initial backfill
            Timestamp::from_secs(now.as_u64().saturating_sub(60 * 60 * 48))
        };
        let until = now;

        // Working group id for fetch/processing; prefer wire id from stored metadata if available
        let gid_for_fetch = if let Some(meta) = group_metadata {
            meta.group_id.clone() // wire id used on relay 'h' tag
        } else {
            group_id.to_string()
        };
        // 2) Build filter for MLS wrapper events (Kind 445) with 'h' tag = gid_for_fetch
        let client = NOSTR_CLIENT.get().ok_or(MlsError::NotInitialized)?;
        let group_id_len = gid_for_fetch.len();
        if group_id_len != 32 && group_id_len != 64 {
            eprintln!(
                "[MLS] sync_group_since_cursor: unsupported group_id length {} for id={}; skipping",
                group_id_len,
                gid_for_fetch
            );
            return Ok((0, 0));
        }

        // Canonical group: safe to use relay-side h-tag filter
        let mut filter = Filter::new()
            .kind(Kind::MlsGroupMessage)
            .since(since)
            .until(until)
            .custom_tag(SingleLetterTag::lowercase(Alphabet::H), &gid_for_fetch)
            .limit(1000);

        // 3) Fetch from TRUSTED_RELAY with reasonable timeout
        let mut used_fallback = false;
        let mut events = match client
            .fetch_events_from(
                vec![TRUSTED_RELAY],
                filter.clone(),
                std::time::Duration::from_secs(15),
            )
            .await
        {
            Ok(evts) => evts,
            Err(e) => {
                return Err(MlsError::NetworkError(format!(
                    "fetch MLS events (with h tag) failed: {}",
                    e
                )))
            }
        };

        // Fallback: if zero results, try without 'h' tag
        if events.is_empty() {
            used_fallback = true;

            filter = Filter::new()
                .kind(Kind::MlsGroupMessage)
                .since(since)
                .until(until)
                .limit(1000);

            events = match client
                .fetch_events_from(
                    vec![TRUSTED_RELAY],
                    filter,
                    std::time::Duration::from_secs(15),
                )
                .await
            {
                Ok(evts) => evts,
                Err(e) => {
                    return Err(MlsError::NetworkError(format!(
                        "fetch MLS events (fallback) failed: {}",
                        e
                    )))
                }
            };
        }

        
        if events.is_empty() {
            return Ok((0, 0));
        }

        // 4) Sort by created_at ascending to ensure deterministic processing
        let mut ordered: Vec<nostr_sdk::Event> = events.into_iter().collect();
        ordered.sort_by_key(|e| e.created_at.as_u64());

        // 4b) If we had to fall back to a broad fetch (no 'h' tag in the filter),
        // first, log observed 'h' tags to verify encoding; then, ONLY IF we positively match, filter.
        if used_fallback {
            // Attempt to filter only if we observe any h tag; otherwise, do not filter and rely on engine.
            let saw_any_h = ordered
                .iter()
                .any(|ev| ev.tags.find(TagKind::SingleLetter(SingleLetterTag::lowercase(Alphabet::H))).is_some());
            if saw_any_h {
                // Try to narrow to our group via h-tag; if none match, proceed unfiltered and let engine decide.
                let original = ordered.clone();
                let filtered: Vec<nostr_sdk::Event> = original
                    .into_iter()
                    .filter(|ev| {
                        match ev.tags.find(TagKind::SingleLetter(SingleLetterTag::lowercase(Alphabet::H))) {
                            Some(tag) => tag.content().map(|s| s == gid_for_fetch).unwrap_or(false),
                            None => false,
                        }
                    })
                    .collect();

                if !filtered.is_empty() {
                    ordered = filtered;
                }
            }
        }

        // 5) Process with persistent engine in a no-await scope
        let mut processed: u32 = 0;
        let mut new_msgs: u32 = 0;
        let mut last_seen_id: Option<nostr_sdk::EventId> = None;
        let mut last_seen_at: u64 = 0;
        
        // Buffer for rumor events to process after engine scope
        let mut rumors_to_process: Vec<(RumorEvent, String, bool)> = Vec::new(); // (rumor, wrapper_id, is_mine)
        
        // Track if we were evicted from this group
        let mut was_evicted = false;
        
        // Resolve my pubkey before entering engine scope (for mine flag)
        let my_pubkey_hex = if let Ok(signer) = client.signer().await {
            if let Ok(my_pubkey) = signer.get_public_key().await {
                my_pubkey.to_hex()
            } else {
                String::new()
            }
        } else {
            String::new()
        };
        
        // Read group metadata before entering engine scope
        let group_check_id = if let Ok(groups) = self.read_groups().await {
            if let Some(meta) = groups.iter().find(|g| g.group_id == gid_for_fetch || g.engine_group_id == gid_for_fetch) {
                // Use the engine_group_id for checking
                if !meta.engine_group_id.is_empty() {
                    Some(meta.engine_group_id.clone())
                } else {
                    Some(meta.group_id.clone())
                }
            } else {
                None
            }
        } else {
            None
        };

        {
            let engine = self.engine()?; // Arc<...> held in scope without awaits
            
            if let Some(ref check_id) = group_check_id {
                // Try to verify if the engine knows about this group
                // We'll attempt to create a dummy message to see if the group exists
                let check_gid_bytes = match hex::decode(&check_id) {
                    Ok(bytes) => bytes,
                    Err(_) => {
                        eprintln!("[MLS] Invalid group_id hex for engine check: {}", check_id);
                        vec![]
                    }
                };
                
                if !check_gid_bytes.is_empty() {
                    use nostr_sdk::prelude::*;
                    let check_gid = GroupId::from_slice(&check_gid_bytes);
                    let dummy_rumor = EventBuilder::new(Kind::Custom(9), "engine_check")
                        .build(nostr_sdk::PublicKey::from_hex("000000000000000000000000000000000000000000000000000000000000dead").unwrap());
                    
                    if let Err(e) = engine.create_message(&check_gid, dummy_rumor) {
                        eprintln!("[MLS] Engine missing group: {}", e);
                        
                        if let Some(handle) = TAURI_APP.get() {
                            handle.emit("mls_group_needs_rejoin", serde_json::json!({
                                "group_id": gid_for_fetch,
                                "reason": "Group not found in MLS engine state"
                            })).ok();
                        }
                    }
                }
            }
            
            for ev in ordered.iter() {
                // Hard guard: only process/persist wrappers whose 'h' tag matches our target group's wire id.
                // This prevents cross-contamination when the fallback fetch returns events for other groups.
                if let Some(tag) = ev.tags.find(TagKind::SingleLetter(SingleLetterTag::lowercase(Alphabet::H))) {
                    if let Some(h_val) = tag.content() {
                        if h_val != gid_for_fetch {
                            // Skip silently - this is expected when multiple groups exist
                            continue;
                        }
                    } else {
                        // Skip silently - empty h tag
                        continue;
                    }
                } else {
                    // Skip silently - no h tag
                    continue;
                }

                match engine.process_message(ev) {
                    Ok(res) => {
                        // Log what type of message we got
                        match res {
                            MessageProcessingResult::ApplicationMessage(msg) => {
                                // Convert MLS ApplicationMessage to RumorEvent for protocol-agnostic processing
                                let rumor_event = RumorEvent {
                                    id: msg.id,
                                    kind: msg.kind,
                                    content: msg.content.clone(),
                                    tags: msg.tags.clone(),
                                    created_at: msg.created_at,
                                    pubkey: msg.pubkey,
                                };

                                let is_mine = !my_pubkey_hex.is_empty() && msg.pubkey.to_hex() == my_pubkey_hex;
                                let wrapper_id = msg.wrapper_event_id.to_hex();

                                // Buffer the rumor for async processing after engine scope
                                rumors_to_process.push((rumor_event, wrapper_id, is_mine));
                                new_msgs = new_msgs.saturating_add(1);
                            }
                            MessageProcessingResult::Commit { mls_group_id: _ } => {
                                // Commit processed - member list may have changed
                                // Check if we're still a member of this group
                                // Use group_check_id (engine's group_id) instead of gid_for_fetch (wrapper id)
                                if let Some(ref check_id) = group_check_id {
                                    let check_gid_bytes = hex::decode(check_id).unwrap_or_default();
                                    if !check_gid_bytes.is_empty() {
                                        let check_gid = GroupId::from_slice(&check_gid_bytes);
                                        let my_pk = nostr_sdk::PublicKey::from_hex(&my_pubkey_hex).ok();
                                        
                                        let still_member = if let Some(pk) = my_pk {
                                            engine.get_members(&check_gid)
                                                .ok()
                                                .map(|members| members.contains(&pk))
                                                .unwrap_or(false)
                                        } else {
                                            false
                                        };
                                        
                                        if !still_member {
                                            // We've been removed from the group!
                                            if let Some(handle) = TAURI_APP.get() {
                                                handle.emit("mls_group_left", serde_json::json!({
                                                    "group_id": gid_for_fetch
                                                })).ok();
                                            }
                                        } else {
                                            // Still a member, just update the UI
                                            if let Some(handle) = TAURI_APP.get() {
                                                handle.emit("mls_group_updated", serde_json::json!({
                                                    "group_id": gid_for_fetch
                                                })).ok();
                                            }
                                        }
                                    }
                                }
                                
                                processed = processed.saturating_add(1);
                            }
                            MessageProcessingResult::Proposal(_proposal) => {
                                // Proposal received (e.g., leave proposal)
                                // Emit event to notify UI that group state may have changed
                                if let Some(handle) = TAURI_APP.get() {
                                    handle.emit("mls_group_updated", serde_json::json!({
                                        "group_id": gid_for_fetch
                                    })).ok();
                                }
                                
                                processed = processed.saturating_add(1);
                            }
                            MessageProcessingResult::ExternalJoinProposal { mls_group_id: _ } => {
                                // No-op for local message persistence
                            }
                            MessageProcessingResult::Unprocessable { mls_group_id: _ } => {
                                // Log unprocessable events for debugging
                                eprintln!("[MLS] Unprocessable event: id={}, created_at={}",
                                         ev.id.to_hex(), ev.created_at.as_u64());
                            }
                        }
                
                        processed = processed.saturating_add(1);
                
                        last_seen_id = Some(ev.id);
                        last_seen_at = ev.created_at.as_u64();
                    }
                    Err(e) => {
                        let error_msg = e.to_string();
                        
                        // Check if this is an eviction error (user was removed from group)
                        if error_msg.contains("own leaf not found") ||
                           error_msg.contains("after being evicted") ||
                           error_msg.contains("evicted from it") {
                            eprintln!("[MLS] ⚠️  EVICTION DETECTED - We were removed from group: {}", gid_for_fetch);
                            
                            // Set flag to remove this group after engine scope
                            was_evicted = true;
                        } else if !error_msg.contains("group not found") {
                            eprintln!(
                                "[MLS] process_message failed (group_id={}, id={}): {}",
                                gid_for_fetch,
                                ev.id,
                                e
                            );
                        }
                        // Continue processing subsequent events
                    }
                }
            }
        } // engine dropped here before any await

        // Process buffered rumors and persist after engine scope ends using unified Chat storage
        // BUT: Skip if we were evicted from this group during sync
        if !rumors_to_process.is_empty() && !was_evicted {
            // Get or create the MLS group chat in STATE with metadata
            let chat_id = {
                let mut state = STATE.lock().await;
                
                // Get group metadata to populate Chat metadata
                let group_meta = self.read_groups().await.ok()
                    .and_then(|groups| groups.into_iter().find(|g| g.group_id == gid_for_fetch));
                
                // Create or get the chat
                let chat_id = state.create_or_get_mls_group_chat(&gid_for_fetch, vec![]);
                
                // Update metadata if we have group info
                let mut metadata_updated = false;
                if let Some(meta) = group_meta {
                    if let Some(chat) = state.get_chat_mut(&chat_id) {
                        chat.metadata.set_name(meta.name.clone());
                        metadata_updated = true;
                    }
                }
                
                // Save chat to disk if metadata was updated
                if metadata_updated {
                    if let Some(handle) = TAURI_APP.get() {
                        let handle_clone = handle.clone();
                        if let Some(chat) = state.get_chat(&chat_id) {
                            if let Err(e) = save_chat(handle_clone, chat).await {
                                eprintln!("[MLS] Failed to save chat after metadata update: {}", e);
                            }
                        }
                    }
                }
                
                chat_id
            };
            
            for (rumor_event, _wrapper_id, is_mine) in rumors_to_process.iter() {
                let rumor_context = RumorContext {
                    sender: rumor_event.pubkey,
                    is_mine: *is_mine,
                    conversation_id: gid_for_fetch.clone(),
                    conversation_type: ConversationType::MlsGroup,
                };

                // Process the rumor using our protocol-agnostic processor
                match process_rumor(rumor_event.clone(), rumor_context).await {
                    Ok(result) => {
                        match result {
                            RumorProcessingResult::TextMessage(msg) | RumorProcessingResult::FileAttachment(msg) => {
                                // Add message to the unified Chat storage
                                let was_added = {
                                    let mut state = STATE.lock().await;
                                    state.add_message_to_chat(&chat_id, msg.clone())
                                };

                                if was_added {
                                    // Emit UI event for new message
                                    if let Some(handle) = TAURI_APP.get() {
                                        handle.emit("mls_message_new", serde_json::json!({
                                            "group_id": gid_for_fetch,
                                            "message": msg
                                        })).unwrap_or_else(|e| {
                                            eprintln!("[MLS] Failed to emit mls_message_new event: {}", e);
                                        });
                                    }
                                }
                            }
                            RumorProcessingResult::Reaction(reaction) => {
                                // Reactions now work with unified storage!
                                let mut state = STATE.lock().await;
                                if let Some((chat_id, msg)) = state.find_chat_and_message_mut(&reaction.reference_id) {
                                    msg.add_reaction(reaction.clone(), Some(chat_id));
                                }
                            }
                            RumorProcessingResult::TypingIndicator { profile_id, until } => {
                                let profile_short: String = profile_id.chars().take(16).collect();
                                println!("[TYPING] 🔄 Processing MLS typing indicator: profile={}, until={}", profile_short, until);
                                
                                // Update the chat's typing participants
                                let active_typers = {
                                    let mut state = STATE.lock().await;
                                    if let Some(chat) = state.get_chat_mut(&chat_id) {
                                        chat.update_typing_participant(profile_id.clone(), until);
                                        let typers = chat.get_active_typers();
                                        println!("[TYPING] 💾 Updated chat state: group={}, active_typers={:?}",
                                            gid_for_fetch.chars().take(8).collect::<String>(), typers);
                                        typers
                                    } else {
                                        println!("[TYPING] ⚠️  Chat not found for typing update: {}", chat_id);
                                        vec![]
                                    }
                                };
                                
                                // Emit typing update event to frontend
                                if let Some(handle) = TAURI_APP.get() {
                                    let _ = handle.emit("typing-update", serde_json::json!({
                                        "conversation_id": gid_for_fetch,
                                        "typers": active_typers,
                                    }));
                                    println!("[TYPING] 📡 Emitted typing-update event: conversation={}, typers_count={}",
                                        gid_for_fetch.chars().take(8).collect::<String>(), active_typers.len());
                                } else {
                                    println!("[TYPING] ⚠️  Failed to get app handle for event emission");
                                }
                            }
                            RumorProcessingResult::Ignored => {
                                // Rumor was ignored (e.g., unknown kind)
                            }
                        }
                    }
                    Err(e) => {
                        eprintln!("[MLS] Failed to process rumor: {}", e);
                    }
                }
            }
            
            // Persist the chat and its messages using unified storage
            if let Some(handle) = TAURI_APP.get() {
                let state = STATE.lock().await;
                if let Some(chat) = state.get_chat(&chat_id) {
                    // Save chat metadata
                    let _ = save_chat(handle.clone(), chat).await;
                    // Save all messages
                    let _ = save_chat_messages(handle.clone(), &chat_id, &chat.messages).await;
                }
            }
        }

        // 6) Clean up if we were evicted from the group
        if was_evicted {
            // Remove cursor for this group (will be reset if re-invited)
            cursors.remove(&gid_for_fetch);
            if let Err(e) = self.write_event_cursors(&cursors).await {
                eprintln!("[MLS] Failed to remove cursor for evicted group: {}", e);
            }
            
            // Perform full cleanup using the helper method
            if let Err(e) = self.cleanup_evicted_group(&gid_for_fetch).await {
                eprintln!("[MLS] Failed to cleanup evicted group: {}", e);
            }
        } else {
            // 7) Advance cursor if anything processed (only if not evicted)
            if processed > 0 {
                if let Some(id) = last_seen_id {
                    cursors.insert(
                        gid_for_fetch.clone(),
                        EventCursor {
                            last_seen_event_id: id.to_hex(),
                            last_seen_at,
                        },
                    );
                    // Persist updated cursors
                    if let Err(e) = self.write_event_cursors(&cursors).await {
                        eprintln!("[MLS] write_event_cursors failed: {}", e);
                    }
                }
            }
        }

        Ok((processed, new_msgs))
    }

    /// Clean up an evicted group (mark as evicted, remove from STATE, delete from DB)
    /// This can be called from both sync and live subscription handlers
    pub async fn cleanup_evicted_group(&self, group_id: &str) -> Result<(), MlsError> {
        // 1. Mark group as evicted in metadata
        let mut groups = self.read_groups().await.unwrap_or_default();
        let mut marked = false;
        for group in &mut groups {
            if group.group_id == group_id || group.engine_group_id == group_id {
                group.evicted = true;
                marked = true;
                break;
            }
        }
        
        if marked {
            if let Err(e) = self.write_groups(&groups).await {
                eprintln!("[MLS] Failed to mark group as evicted: {}", e);
            }
        }
        
        // 2. Remove from in-memory STATE
        {
            let mut state = STATE.lock().await;
            state.chats.retain(|c| c.id() != group_id);
        }
        
        // 3. Delete from database
        if let Some(handle) = TAURI_APP.get() {
            if let Err(e) = crate::db_migration::delete_chat(handle.clone(), group_id).await {
                eprintln!("[MLS] Failed to delete chat from storage: {}", e);
            }
        }
        
        // 4. Emit event to frontend
        if let Some(handle) = TAURI_APP.get() {
            if let Err(e) = handle.emit("mls_group_left", serde_json::json!({
                "group_id": group_id
            })) {
                eprintln!("[MLS] Failed to emit mls_group_left event: {}", e);
            }
        }
        
        Ok(())
    }

    // Internal helper methods for store access
    // These follow the read/modify/write pattern used in the codebase
    
    /// Read and decrypt group metadata from store
    pub async fn read_groups(&self) -> Result<Vec<MlsGroupMetadata>, MlsError> {
        // Read and decrypt "mls_groups" from JSON store
        let handle = TAURI_APP.get().ok_or(MlsError::NotInitialized)?.clone();
        let store = db::get_store(&handle);

        let encrypted_opt = match store.get("mls_groups") {
            Some(v) if v.is_string() => Some(v.as_str().unwrap().to_string()),
            _ => None,
        };

        if let Some(enc) = encrypted_opt {
            let json = crypto::internal_decrypt(enc, None)
                .await
                .map_err(|_| MlsError::CryptoError("decrypt mls_groups".into()))?;
            let groups: Vec<MlsGroupMetadata> = serde_json::from_str(&json)
                .map_err(|e| MlsError::StorageError(format!("deserialize mls_groups: {}", e)))?;
            Ok(groups)
        } else {
            Ok(Vec::new())
        }
    }

    /// Write encrypted group metadata to store
    pub async fn write_groups(&self, groups: &[MlsGroupMetadata]) -> Result<(), MlsError> {
        // Serialize, encrypt and write "mls_groups"
        let handle = TAURI_APP.get().ok_or(MlsError::NotInitialized)?.clone();
        let store = db::get_store(&handle);

        let json = serde_json::to_string(groups)
            .map_err(|e| MlsError::StorageError(format!("serialize mls_groups: {}", e)))?;
        let encrypted = crypto::internal_encrypt(json, None).await;

        store.set("mls_groups".to_string(), serde_json::json!(encrypted));
        Ok(())
    }

    /// Read keypackage index from store
    #[allow(dead_code)]
    async fn read_keypackage_index(&self) -> Result<Vec<KeyPackageIndexEntry>, MlsError> {
        // Plaintext read of "mls_keypackage_index"
        let handle = TAURI_APP.get().ok_or(MlsError::NotInitialized)?.clone();
        let store = db::get_store(&handle);

        let index: Vec<KeyPackageIndexEntry> = match store.get("mls_keypackage_index") {
            Some(v) => serde_json::from_value(v.clone()).unwrap_or_default(),
            None => Vec::new(),
        };

        Ok(index)
    }

    /// Write keypackage index to store
    #[allow(dead_code)]
    async fn write_keypackage_index(&self, index: &[KeyPackageIndexEntry]) -> Result<(), MlsError> {
        // Plaintext write to "mls_keypackage_index"
        let handle = TAURI_APP.get().ok_or(MlsError::NotInitialized)?.clone();
        let store = db::get_store(&handle);

        let value = serde_json::to_value(index)
            .map_err(|e| MlsError::StorageError(format!("serialize keypackage_index: {}", e)))?;
        store.set("mls_keypackage_index".to_string(), value);
        Ok(())
    }

    /// Read event cursors from store
    #[allow(dead_code)]
    pub async fn read_event_cursors(&self) -> Result<HashMap<String, EventCursor>, MlsError> {
        // Plaintext read of "mls_event_cursors"
        let handle = TAURI_APP.get().ok_or(MlsError::NotInitialized)?.clone();
        let store = db::get_store(&handle);

        let cursors: HashMap<String, EventCursor> = match store.get("mls_event_cursors") {
            Some(v) => serde_json::from_value(v.clone()).unwrap_or_default(),
            None => HashMap::new(),
        };

        Ok(cursors)
    }

    /// Write event cursors to store
    #[allow(dead_code)]
    pub async fn write_event_cursors(&self, cursors: &HashMap<String, EventCursor>) -> Result<(), MlsError> {
        // Plaintext write to "mls_event_cursors"
        let handle = TAURI_APP.get().ok_or(MlsError::NotInitialized)?.clone();
        let store = db::get_store(&handle);

        let value = serde_json::to_value(cursors)
            .map_err(|e| MlsError::StorageError(format!("serialize event_cursors: {}", e)))?;
        store.set("mls_event_cursors".to_string(), value);
        Ok(())
    }
    
    /// Run an in-memory MLS smoke test with the provided Nostr client
    ///
    /// This is a network-only smoke test that validates basic MLS operations
    /// without persisting any state to disk. It performs the following:
    /// - Publishes Saul's device KeyPackage
    /// - Creates a temporary group (Kim creator, Saul member; Kim admin)
    /// - Sends one MLS application message
    /// - Observes the wrapper on the relay
    ///
    /// All operations are wrapped in a timeout to prevent hanging.
    pub async fn run_mls_smoke_test_with_client(
        client: &nostr_sdk::Client,
        relay: &str,
        timeout: std::time::Duration,
    ) -> Result<(), MlsError> {
        use nostr_sdk::prelude::*;

        match tokio::time::timeout(timeout, async {
            println!("[MLS Smoke Test] Start (relay: {})", relay);

            // Use two ephemeral identities (do NOT use the logged-in client's keys)
            use nostr_sdk::prelude::Keys;
            let kim_keys = Keys::generate();
            let saul_keys = Keys::generate();
            println!(
                "[MLS Smoke Test] Ephemeral identities: kim={}, saul={}",
                kim_keys.public_key().to_bech32().unwrap_or_default(),
                saul_keys.public_key().to_bech32().unwrap_or_default()
            );

            // Two independent in-memory MLS engines (no disk I/O)
            let kim_mls = MDK::new(MdkSqliteStorage::new(":memory:").map_err(|e| MlsError::StorageError(e.to_string()))?);
            let saul_mls = MDK::new(MdkSqliteStorage::new(":memory:").map_err(|e| MlsError::StorageError(e.to_string()))?);

            // RelayUrl (nostr-mls type)
            let relay_url = RelayUrl::parse(relay)
                .map_err(|e| MlsError::NetworkError(format!("RelayUrl::parse: {}", e)))?;

            // 1) Saul publishes a device KeyPackage (so Kim can add him)
            println!("[MLS Smoke Test] Saul publishing device KeyPackage...");
            let (saul_kp_encoded, saul_kp_tags) = saul_mls
                .create_key_package_for_event(&saul_keys.public_key(), [relay_url.clone()])
                .map_err(|e| MlsError::NostrMlsError(format!("create_key_package_for_event (saul): {}", e)))?;
    
            // Build + sign with Saul's ephemeral keys, then publish with the app's client
            let saul_kp_event = EventBuilder::new(Kind::MlsKeyPackage, saul_kp_encoded)
                .tags(saul_kp_tags)
                .build(saul_keys.public_key())
                .sign(&saul_keys)
                .await
                .map_err(|e| MlsError::NostrMlsError(format!("sign saul keypackage: {}", e)))?;
            client
                .send_event_to([relay], &saul_kp_event)
                .await
                .map_err(|e| MlsError::NetworkError(format!("publish saul keypackage: {}", e)))?;
            println!("[MLS Smoke Test] Saul KeyPackage published id={}", saul_kp_event.id);

            // 2) Kim creates a temporary two-member group (Kim creator + Saul member)
            println!("[MLS Smoke Test] Kim creating temporary group with Saul as member...");
            let name = format!(
                "Vector-MLS-Test-{}",
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_secs()
            );
            let description = "Vector MLS in-memory smoke test (Kim+Saul)".to_owned();
    
            let group_config = NostrGroupConfigData::new(
                name,
                description,
                None, // image_hash
                None, // image_key
                None, // image_nonce
                vec![relay_url.clone()],
                vec![kim_keys.public_key()], // admins - moved from create_group call
            );
    
            // IMPORTANT: Non-empty member_key_package_events (Saul). The creator (Kim) must not be in member_key_package_events.
            let group_create = kim_mls
                .create_group(
                    &kim_keys.public_key(),
                    vec![saul_kp_event.clone()],      // Saul invited via his KeyPackage
                    group_config,                     // admins now in config
                )
                .map_err(|e| MlsError::NostrMlsError(format!("create_group (kim): {}", e)))?;
    
            let kim_group = group_create.group;
            let welcome_rumor = group_create
                .welcome_rumors
                .first()
                .cloned()
                .ok_or_else(|| MlsError::NostrMlsError("no welcome rumor produced".into()))?;
            println!("[MLS Smoke Test] Group created; welcome rumor produced");
    
            // 2b) Saul processes the welcome locally and joins (no network, purely in-memory)
            saul_mls
                .process_welcome(&nostr_sdk::EventId::all_zeros(), &welcome_rumor)
                .map_err(|e| MlsError::NostrMlsError(format!("saul process_welcome: {}", e)))?;
            let welcomes = saul_mls
                .get_pending_welcomes()
                .map_err(|e| MlsError::NostrMlsError(format!("saul get_pending_welcomes: {}", e)))?;
            let welcome = welcomes
                .first()
                .cloned()
                .ok_or_else(|| MlsError::NostrMlsError("saul has no pending welcomes".into()))?;
            saul_mls
                .accept_welcome(&welcome)
                .map_err(|e| MlsError::NostrMlsError(format!("saul accept_welcome: {}", e)))?;
            println!("[MLS Smoke Test] Saul joined the group locally");

            // 3) Kim sends an MLS application message and publishes the wrapper to the relay
            let group_id = &kim_group.mls_group_id; // Already a GroupId in MDK
            println!("[MLS Smoke Test] Kim sending application message...");
            let rumor = EventBuilder::new(Kind::PrivateDirectMessage, "Vector-MLS-Test: hello")
                .tag(Tag::custom(
                    TagKind::Custom(std::borrow::Cow::Borrowed("vector-mls-test")),
                    vec!["true"],
                ))
                .build(kim_keys.public_key());
    
            let mls_wrapper = kim_mls
                .create_message(&group_id, rumor)
                .map_err(|e| MlsError::NostrMlsError(format!("kim create_message: {}", e)))?;
    
            client
                .send_event_to([relay], &mls_wrapper)
                .await
                .map_err(|e| MlsError::NetworkError(format!("publish mls wrapper: {}", e)))?;
            println!(
                "[MLS Smoke Test] MLS wrapper published id={}, kind={:?}",
                mls_wrapper.id,
                mls_wrapper.kind
            );

            // 4) Verify network visibility once and then process locally on Saul
            let filter = Filter::new()
                .kind(Kind::MlsGroupMessage)
                .since(Timestamp::now() - 300u64); // widen observation window (5 minutes)

            let fetched = client
                .fetch_events_from(
                    vec![relay.to_string()],
                    filter,
                    std::time::Duration::from_secs(10),
                )
                .await
                .map_err(|e| MlsError::NetworkError(format!("fetch MLS events: {}", e)))?;

            if fetched.iter().any(|e| e.id == mls_wrapper.id) {
                println!("[MLS Smoke Test] Observed wrapper on relay");
                println!("[MLS Smoke Test] Saul processing locally after relay observation...");
            } else {
                println!("[MLS Smoke Test] Wrapper not observed in single fetch window; processing locally anyway...");
            }
    
            match saul_mls.process_message(&mls_wrapper) {
                Ok(_res) => println!("[MLS Smoke Test] Saul process_message => OK"),
                Err(e) => println!("[MLS Smoke Test] Saul process_message note: {}", e),
            }

            println!("[MLS Smoke Test] Completed in-memory smoke test (Kim+Saul, no disk).");
            Ok(())
        })
        .await
        {
            Ok(r) => r,
            Err(_) => Err(MlsError::NetworkError(format!(
                "MLS smoke test timed out after {}s",
                timeout.as_secs()
            ))),
        }
    }
}

/// Send an MLS message (rumor) to a group
///
/// This function takes a group_id and an UnsignedEvent (rumor) and sends it through the MLS protocol.
/// It's used by the protocol-agnostic message sending system to route group messages through MLS.
pub async fn send_mls_message(group_id: &str, rumor: nostr_sdk::UnsignedEvent) -> Result<(), String> {
    let group_id = group_id.to_string();
    
    // Run non-Send MLS engine work on blocking thread
    tokio::task::spawn_blocking(move || {
        let handle = TAURI_APP.get()
            .ok_or_else(|| "App handle not initialized".to_string())?
            .clone();
        
        let rt = tokio::runtime::Handle::current();
        rt.block_on(async move {
            // Get the Nostr client
            let client = NOSTR_CLIENT.get()
                .ok_or_else(|| "Nostr client not initialized".to_string())?;
            
            // Create MLS service instance
            let service = MlsService::new_persistent(&handle)
                .map_err(|e| format!("Failed to create MLS service: {}", e))?;
            
            // Look up the group to get the engine_group_id (do this before getting engine)
            let groups = service.read_groups().await
                .map_err(|e| format!("Failed to read groups: {}", e))?;
            
            let group_meta = groups.iter()
                .find(|g| g.group_id == group_id)
                .ok_or_else(|| format!("Group not found: {}", group_id))?;
            
            // Parse the engine group ID
            let engine_group_id = if group_meta.engine_group_id.is_empty() {
                return Err("Group has no engine_group_id".to_string());
            } else {
                GroupId::from_slice(
                    &hex::decode(&group_meta.engine_group_id)
                        .map_err(|e| format!("Invalid engine_group_id hex: {}", e))?
                )
            };
            
            // Now get the MLS engine and create message (no await while engine is in scope)
            let mls_wrapper_result = {
                let engine = service.engine()
                    .map_err(|e| format!("Failed to get MLS engine: {}", e))?;
                
                engine.create_message(&engine_group_id, rumor.clone())
            }; // engine dropped here
            
            // Check for eviction errors after engine is dropped
            let mls_wrapper = match mls_wrapper_result {
                Ok(wrapper) => wrapper,
                Err(e) => {
                    let error_msg = e.to_string();
                    
                    // Check if this is an eviction error
                    if error_msg.contains("own leaf not found") ||
                       error_msg.contains("after being evicted") ||
                       error_msg.contains("evicted from it") ||
                       error_msg.contains("group not found") {
                        eprintln!("[MLS] Eviction detected while sending to group: {}", group_id);
                        
                        // Perform cleanup (we're in an async context now)
                        if let Err(cleanup_err) = service.cleanup_evicted_group(&group_id).await {
                            eprintln!("[MLS] Failed to cleanup evicted group: {}", cleanup_err);
                        }
                    }
                    
                    return Err(format!("Failed to create MLS message: {}", e));
                }
            };
            
            // Check if this is a typing indicator and add expiration to wrapper if so
            let is_typing_indicator = rumor.kind == nostr_sdk::Kind::ApplicationSpecificData
                && rumor.content == "typing";
            
            if is_typing_indicator {
                // For typing indicators, add a 30-second expiration to the wrapper event
                use nostr_sdk::{EventBuilder, Tag, Timestamp};
                
                let expiry_time = Timestamp::from_secs(
                    std::time::SystemTime::now()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs()
                        + 30,
                );
                
                // Create a new wrapper event with expiration tag
                let mut wrapper_builder = EventBuilder::new(mls_wrapper.kind, &mls_wrapper.content);
                
                // Copy all existing tags
                for tag in mls_wrapper.tags.iter() {
                    wrapper_builder = wrapper_builder.tag(tag.clone());
                }
                
                // Add expiration tag
                wrapper_builder = wrapper_builder.tag(Tag::expiration(expiry_time));
                
                // Build and sign the wrapper
                let signer = client.signer().await
                    .map_err(|e| format!("Failed to get signer: {}", e))?;
                let wrapper_with_expiry = wrapper_builder.sign(&signer).await
                    .map_err(|e| format!("Failed to sign wrapper with expiration: {}", e))?;
                
                // Send the wrapper with expiration
                client
                    .send_event_to([TRUSTED_RELAY], &wrapper_with_expiry)
                    .await
                    .map_err(|e| format!("Failed to send MLS wrapper: {}", e))?;
            } else {
                // Send normal wrapper without expiration
                client
                    .send_event_to([TRUSTED_RELAY], &mls_wrapper)
                    .await
                    .map_err(|e| format!("Failed to send MLS wrapper: {}", e))?;
            }
            
            Ok(())
        })
    })
    .await
    .map_err(|e| format!("Task join error: {}", e))?
}